---
title: "Capstone Project: Google Data Analytics Professional Certificate"
author: "Lucas Heyden"
date: "2025-03-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
### About the Company

Bellabeat is a technology company founded by Urška Sršen and Sando Mur, specializing in the manufacture of health-focused products for women. Since its establishment in 2013, Bellabeat has rapidly grown, positioning itself as an innovative brand in the smart device market. The company collects data on activity, sleep, stress, and reproductive health, empowering women to better understand their habits and make healthy decisions. With a strong digital presence, Bellabeat invests in online marketing, including ads on Google, Facebook, Instagram, and YouTube, as well as maintaining an e-commerce channel.

  

### Products

Bellabeat offers a range of products designed to monitor the health and well-being of users. These include:

- **Bellabeat App**: Provides health data related to activity, sleep, stress, menstrual cycle, and mindfulness habits.

- **Leaf**: A wellness tracker that can be worn as a bracelet, necklace, or clip, connecting to the app to track activity, sleep, and stress.

- **Time**: A wellness watch that combines classic design with smart technology to monitor activity, sleep, and stress.

- **Spring**: A water bottle that tracks daily water intake, ensuring users stay hydrated.

- **Bellabeat Subscription**: A membership program offering personalized guidance on nutrition, activity, sleep, health and beauty, and mindfulness.

  

### Stakeholders

The key stakeholders of Bellabeat include:

- **Urška Sršen**: Co-founder and Creative Director, who believes in the potential of data analysis to drive the company's growth.

- **Sando Mur**: Co-founder and mathematician, a key member of Bellabeat's executive team.

- **Marketing Analysis Team**: A team of data analysts responsible for collecting, analyzing, and reporting data that helps guide Bellabeat's marketing strategy. As a junior analyst, you are part of this team and focused on understanding how consumers use Bellabeat products to provide strategic recommendations.

## Project Deliverables

1. **Clear Summary of the Business Task**: A concise and objective description of the project’s goal, highlighting the objectives and expected outcomes.

2. **Description of All Data Sources Used**: A comprehensive overview of all data sources employed in the analysis, including the origin, nature of the data, and relevance to the business task.

3. **Documentation of Data Cleansing and Manipulation**: A detailed record of all steps taken for data cleaning and manipulation, including techniques used, transformations applied, and justifications for the decisions made.

4. **Summary of the Analysis Conducted**: An overview of the methodologies and analytical approaches employed, highlighting the key steps and techniques used to reach the conclusions.

5. **Supporting Visualizations and Key Findings**: Graphs and visualizations that illustrate the results of the analysis, accompanied by a summary of the most relevant discoveries and insights obtained.

6. **Marketing Recommendations for a Selected Product**: Strategic suggestions based on the analysis, focused on a specific product, aimed at optimizing marketing campaigns and driving sales.


## Data analysis process

### Ask

The co-founder and Creative Director of Bellabeat, Urška Sršen, has requested an analysis of smart device usage data in order to gain insights into how consumers use non-Bellabeat smart devices. Based on these insights, it will be necessary to select one Bellabeat product to apply the findings in the presentation.

The key questions that will guide this analysis are:

- **What are some trends in smart device usage?**
- **How could these trends apply to Bellabeat customers?**
- **How could these trends help influence Bellabeat's marketing strategy?**

These questions are fundamental to understanding consumer behavior and how Bellabeat can position itself more effectively in the market.


### Prepare 
#### Data storage  
This dataset was collected via Amazon Mechanical Turk between March and May 2016. It contains data from 30 Fitbit users who consented to sharing their personal fitness tracker information, including physical activity, heart rate, and sleep. The dataset, publicly available on Kaggle through Mobius, enables analysis of user habits.  

#### Organization  
The dataset consists of 18 CSV files, with 15 in long format and 3 in wide format. Each user has a unique ID, and records are organized by day and hour. The files were processed in RStudio Cloud.  


| File Name                           | Type | Format | Days | Users | Description |
|--------------------------------------|------|--------|------|-------|------------------------------------------------|
| dailyActivity_merged                 | CSV  | Long   | 30   | 33    | Daily activity: steps, distance, intensity, calories |
| dailyCalories_merged                 | CSV  | Long   | 30   | 33    | Daily calorie burn |
| dailyIntensities_merged              | CSV  | Long   | 30   | 33    | Daily intensity levels (minutes and distances) |
| dailySteps_merged                    | CSV  | Long   | 30   | 33    | Total daily step count |
| heartrate_seconds_merged             | CSV  | Long   | 30   | 14    | Heart rate recorded every 5 seconds |
| hourlyCalories_merged                 | CSV  | Long   | 30   | 33    | Hourly calorie burn |
| hourlyIntensities_merged              | CSV  | Long   | 30   | 33    | Hourly intensity levels and averages |
| hourlySteps_merged                    | CSV  | Long   | 30   | 33    | Steps recorded per hour |
| minuteCaloriesNarrow_merged           | CSV  | Long   | 30   | 33    | Calorie burn per minute (one row per minute) |
| minuteCaloriesWide_merged             | CSV  | Wide   | 30   | 33    | Calorie burn per minute (one column per minute) |
| minuteIntensitiesNarrow_merged        | CSV  | Long   | 30   | 33    | Intensity per minute (one row per minute) |
| minuteIntensitiesWide_merged          | CSV  | Wide   | 30   | 33    | Intensity per minute (one column per minute) |
| minuteMETsNarrow_merged               | CSV  | Long   | 30   | 33    | Metabolic equivalent (MET) per minute |
| minuteSleep_merged                     | CSV  | Long   | 31   | 24    | Minute-by-minute sleep records |
| minuteStepsNarrow_merged              | CSV  | Long   | 30   | 33    | Step count per minute (one row per minute) |
| minuteStepsWide_merged                | CSV  | Wide   | 30   | 33    | Step count per minute (one column per minute) |
| sleepDay_merged                        | CSV  | Long   | 30   | 24    | Sleep records: total sleep time and time in bed |
| weightLogInfo_merged                   | CSV  | Long   | 30   | 8     | Weight, body fat percentage, BMI, report type |


#### Credibility  
An evaluation using the **ROCCC** method reveals:  
- **Reliability:** Low, due to a small sample size and short collection period.  
- **Originality:** Collected by third parties (MTurk) rather than directly by Fitbit.  
- **Comprehensiveness:** Limited, as demographic details are missing.  
- **Timeliness:** Outdated (2016), potentially not reflecting current trends.  
- **Citation:** Published on Zenodo.org by Furberg et al., providing some academic credibility.  

#### Licensing & Privacy  
The dataset is in the public domain (CC0), allowing free use. All personal data has been anonymized in compliance with GDPR regulations to ensure user privacy.  

#### Integrity  
*File integrity was verified using hash validation, ensuring no alterations occurred during download.  

#### Limitations  
Variations in unique IDs, differences between automated and manually entered data, and lack of standardization across devices may affect analysis. However, the insights gained can help Bellabeat enhance its products.  



### Process 
Data integrity and the cleaning process are essential in data analysis, ensuring that data is accurate, consistent, and reliable. Data cleaning involves identifying and correcting errors or inconsistencies, which enhances the quality of insights derived from the data. In this discussion, we will explore several critical aspects of data integrity and the data cleaning process. The following points will be addressed:

- The tools selected for data cleaning and the rationale behind those choices.
- Methods employed to ensure the integrity of the data.
- Steps taken to maintain a clean dataset.
- Verification processes to confirm that the data is ready for analysis.
- Documentation of the cleaning process for future reference and sharing.

These elements are vital for ensuring that the data used in analysis is of high quality, ultimately leading to more accurate and actionable insights.

#### Tool used
For the analysis, I will be using R, a powerful and efficient tool for data processing, cleaning, and visualization. R is widely recognized for its extensive libraries and packages, such as `dplyr` for data manipulation and `ggplot2` for creating stunning visualizations. Its ability to handle large datasets and perform complex statistical analyses makes it one of the most popular programming languages in the data science community today. Additionally, R's open-source nature allows for continuous improvement and a vibrant community that contributes to its vast ecosystem of resources. This makes R an ideal choice for ensuring high-quality data analysis.

#### Loading libraries and importing files
```{r cars}
library(tidyverse)
library(lubridate)
library(ggplot2)
library(ggrepel)
library(readr)
library(dplyr)
library(tidyr)
library(here)
library(skimr)
library(janitor)
library(corrplot)
library(ggcorrplot)
library(cluster)
library(factoextra)

df_daily_activity <- read_csv("/cloud/project/raw_data_bellabeat/dailyActivity_merged.csv")
df_daily_calories <- read_csv("/cloud/project/raw_data_bellabeat/dailyCalories_merged.csv")
df_daily_intensities <- read_csv("/cloud/project/raw_data_bellabeat/dailyIntensities_merged.csv")
df_daily_steps <- read_csv("/cloud/project/raw_data_bellabeat/dailySteps_merged.csv")
df_daily_sleep <- read_csv("/cloud/project/raw_data_bellabeat/sleepDay_merged.csv")





```

#### Exploratory analisys
```{r}
str(df_daily_activity)

str(df_daily_sleep)

str(df_daily_intensities)

str(df_daily_calories)

str(df_daily_steps)
```

#### Data preparation

***
**Daily Activity**

```{r}
df_daily_activity$ActivityDate <- as.Date(df_daily_activity$ActivityDate, format = "%m/%d/%Y")
df_daily_activity$Id <- as.character(df_daily_activity$Id)
df_daily_activity <- df_daily_activity %>% clean_names()
```

Each Id corresponds to a user, so the Id is not unique in the dataframe, and it is not possible to use it as a primary key for linking or checking for duplication.
```{r}
n_distinct(df_daily_activity$id) == nrow(df_daily_activity)

n_distinct(df_daily_activity$id)
nrow(df_daily_activity)
```

As it was verified that the Id is not unique, a uniqueness and non-null validation will be performed based on the column values.

```{r}
dp_daily_activity <- sum(duplicated(df_daily_activity)) * 100 / nrow(df_daily_activity)
na_daily_activity <- sum(is.na(df_daily_activity)) * 100 / nrow(df_daily_activity)

dp_daily_activity
na_daily_activity
```

Since there are no duplicate rows in the dataset, there is no need to remove any. So, next, the 'Id' column will be renamed to fk_daily_activity so that it can be used as a foreign key if needed, and a new Id column will be generated which is the concatenation of fk_daily_activity and activity_date

```{r}
df_daily_activity <- df_daily_activity %>%
  rename(fk_daily_activity = id)

df_daily_activity <- df_daily_activity %>%
  mutate(id = paste(fk_daily_activity, activity_date, sep = "_"))

df_daily_activity <- df_daily_activity %>%
  relocate(id, .before = fk_daily_activity)

n_distinct(df_daily_activity$id) == nrow(df_daily_activity)

n_distinct(df_daily_activity$id)
nrow(df_daily_activity)
```

Creation of the difference column between the total distance and the distance tracked by the application.

```{r}
df_daily_activity <- df_daily_activity %>%
  mutate(delta_distances = total_distance - tracker_distance)
```

Assessment of the amount of data by which the actual distance diverges from the tracked distance.
```{r}
df_daily_activity %>%
  mutate(delta_group = case_when(
    delta_distances == 0 ~ "0",
    delta_distances > 0 & delta_distances <= 0.5 ~ "0–0.5",
    delta_distances > 0.5 & delta_distances <= 1 ~ "0.5–1",
    delta_distances > 1 & delta_distances <= 1.5 ~ "1–1.5",
    delta_distances > 1.5 ~ ">1.5"
  )) %>%
  group_by(delta_group) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100, 2)) -> delta_summary
  
delta_summary
```

Since the difference between the actual distance and the distance recorded by the device is greater than zero in less than 2% of the analyzed sample, we will proceed with the analysis excluding these cases. This ensures that the evaluated data reflects more accurate measurements.

```{r}
df_daily_activity <- df_daily_activity %>% filter(delta_distances == 0)
```

Adding weekday number and weekday name.
```{r}
df_daily_activity <- df_daily_activity %>%
  mutate(
    week_day_name = weekdays(activity_date),
    # 0 = Sunday, 1 = Monday, ..., 6 = Saturday
    week_day_number = as.integer(format(activity_date, "%w")) 
  )
```



At the end of processing this was the final dataframe.
```{r}
str(df_daily_activity)
```

Based on this dataframe, df_users_activity will be created, which will show the percentage of days in which each user was active. Considering the OMS recommendation of at least 7,000 steps per day, this will be the criterion adopted to define whether a user had an active day or not.

```{r}
user_active_dates <- df_daily_activity %>%
  filter(total_steps > 7000) %>%
  group_by(fk_daily_activity) %>%
  summarise(active_days = n(), .groups = "drop")


user_all_dates <- df_daily_activity %>%
  group_by(fk_daily_activity) %>%
  summarise(total_days = n(), .groups = "drop")

df_user_activity <- user_all_dates %>%
  left_join(user_active_dates, by = "fk_daily_activity") %>%
  mutate(
    id_user = fk_daily_activity,
    active_days = coalesce(active_days, 0),
    user_activity_in_period = (active_days * 100) / total_days
  ) %>%
  select(id_user, user_activity_in_period) %>%
  arrange(desc(user_activity_in_period)) 



df_user_activity
```


***
**Daily Sleep**

Validation of non-unique and null values.
```{r}
dp_daily_sleep <- sum(duplicated(df_daily_sleep)) * 100 / nrow(df_daily_sleep)
na_daily_sleep <- sum(is.na(df_daily_sleep)) * 100 / nrow(df_daily_sleep)

dp_daily_sleep
na_daily_sleep
```

Since df_daily_sleep has duplicate data and there is little of it, only non-duplicated data will be considered in the analysis.
```{r}
df_daily_sleep <- df_daily_sleep %>% 
  distinct()
```

Separating date and time.
```{r}
df_daily_sleep$Id <- as.character(df_daily_sleep$Id)

df_daily_sleep <- df_daily_sleep %>%
  separate(SleepDay, into = c("sleep_date", "sleep_hour"), sep = " ")

df_daily_sleep$sleep_date <- as.Date(df_daily_sleep$sleep_date, format = "%m/%d/%Y")


df_daily_sleep <- df_daily_sleep %>% clean_names()


str(df_daily_sleep)
```

Creating a new Id, as was done in df_daily_activity.
```{r}
df_daily_sleep <- df_daily_sleep %>%
  rename(fk_daily_sleep = id)

df_daily_sleep <- df_daily_sleep %>%
  mutate(id = paste(fk_daily_sleep, sleep_date, sep = "_"))

df_daily_sleep <- df_daily_sleep %>%
  relocate(id, .before = fk_daily_sleep)

n_distinct(df_daily_sleep$id) == nrow(df_daily_sleep)

n_distinct(df_daily_sleep$id)
nrow(df_daily_sleep)
```

This is the result after processing df_daily_sleep.
```{r}
str(df_daily_sleep)
```

Incorporating average sleep into the dataframe with consolidated user data.
```{r}
df_avg_sleep <- df_daily_sleep %>%
  group_by(fk_daily_sleep) %>%
  summarise(avg_minutes_asleep = mean(total_minutes_asleep, na.rm = TRUE), .groups = "drop") %>%
  rename(id_user = fk_daily_sleep)



df_user_activity <- df_user_activity %>%
  left_join(df_avg_sleep, by = c("id_user" = "id_user")) %>%
  select(id_user, user_activity_in_period, avg_minutes_asleep)

df_user_activity
```


***
**Daily Calories**

Validation of non-unique and null values.
```{r}
dp_daily_calories <- sum(duplicated(df_daily_calories)) * 100 / nrow(df_daily_calories)
na_df_daily_calories <- sum(is.na(df_daily_calories)) * 100 / nrow(df_daily_calories)

dp_daily_calories
na_df_daily_calories

str(df_daily_calories)
```

Processing the dataframe data so that it is standardized.

```{r}
df_daily_calories$ActivityDate <- as.Date(df_daily_calories$ActivityDay, format = "%m/%d/%Y")
df_daily_calories$Id <- as.character(df_daily_calories$Id)
df_daily_calories <- df_daily_calories %>% 
  clean_names() %>%
  select(id, activity_date, calories) %>%
  rename(id_user = id) %>%
  mutate(id = paste(id_user, activity_date, sep = "_")) %>%
  relocate(id, .before = id_user)
df_daily_calories
```

Aggregating average calories burned by users in df_user_activity
```{r}
df_avg_user_calories <- df_daily_calories %>%
  group_by(id_user) %>%
  summarise(avg_calories = mean(calories, na.rm = TRUE))

df_user_activity <- df_user_activity %>%
  left_join(df_avg_user_calories, by = c("id_user" = "id_user")) %>%
  select(id_user, user_activity_in_period, avg_minutes_asleep, avg_calories)

str(df_user_activity)
```


***
**Daily Intensities**
Validation of non-unique and null values.
```{r}
dp_daily_intensities <- sum(duplicated(df_daily_intensities)) * 100 / nrow(df_daily_intensities)
na_daily_intensities <- sum(is.na(df_daily_intensities)) * 100 / nrow(df_daily_intensities)

dp_daily_intensities
na_daily_intensities

str(df_daily_intensities)
```

Processing the dataframe data so that it is standardized.
```{r}
df_daily_intensities$Id <- as.character(df_daily_intensities$Id)
df_daily_intensities <- df_daily_intensities %>% 
  clean_names() %>%
  rename(id_user = id) %>%
  rename(intensities_date = activity_day) %>%
  mutate(id = paste(id_user, intensities_date, sep = "_")) %>%
  relocate(id, .before = id_user)
str(df_daily_intensities)
```


Aggregating average calories burned by users in df_user_activity
```{r}
df_avg_user_intensities <- df_daily_intensities %>%
  group_by(id_user) %>%
  summarise(
    avg_lightly_active_minutes = mean(lightly_active_minutes, na.rm = TRUE),
    avg_fairly_active_minutes = mean(fairly_active_minutes, na.rm = TRUE),
    avg_very_active_minutes = mean(very_active_minutes, na.rm = TRUE),
    avg_sedentary_active_distance = mean(sedentary_active_distance, na.rm = TRUE),
    avg_light_active_distance = mean(light_active_distance, na.rm = TRUE),
    avg_moderately_active_distance = mean(moderately_active_distance, na.rm = TRUE),
    avg_very_active_distance = mean(very_active_distance, na.rm = TRUE)
  )

df_user_activity <- df_user_activity %>%
  left_join(df_avg_user_intensities, by = "id_user") %>%
  select(
    id_user,
    user_activity_in_period,
    avg_minutes_asleep,
    avg_calories,
    avg_lightly_active_minutes,
    avg_fairly_active_minutes,
    avg_very_active_minutes,
    avg_sedentary_active_distance,
    avg_light_active_distance,
    avg_moderately_active_distance,
    avg_very_active_distance
  )

str(df_user_activity)
```

***
**Daily Steps**

Validation of non-unique and null values.
```{r}
dp_daily_steps <- sum(duplicated(df_daily_steps)) * 100 / nrow(df_daily_steps)
na_daily_steps <- sum(is.na(df_daily_steps)) * 100 / nrow(df_daily_steps)

dp_daily_steps
na_daily_steps

str(df_daily_steps)
```

Processing the dataframe data so that it is standardized.
```{r}
df_daily_steps$Id <- as.character(df_daily_steps$Id)
df_daily_steps <- df_daily_steps %>% 
  clean_names() %>%
  rename(id_user = id) %>%
  rename(steps_date = activity_day) %>%
  mutate(id = paste(id_user, steps_date, sep = "_")) %>%
  relocate(id, .before = id_user)

str(df_daily_steps)
```


Aggregating average calories burned by users in df_user_activity
```{r}
df_avg_steps <- df_daily_steps %>%
  group_by(id_user) %>%
  summarise(avg_steps = mean(step_total, na.rm = TRUE))

df_user_activity <- df_user_activity %>%
  left_join(df_avg_steps, by = "id_user") %>%
  select(
    id_user,
    user_activity_in_period,
    avg_minutes_asleep,
    avg_calories,
    avg_lightly_active_minutes,
    avg_fairly_active_minutes,
    avg_very_active_minutes,
    avg_sedentary_active_distance,
    avg_light_active_distance,
    avg_moderately_active_distance,
    avg_very_active_distance,
    avg_steps
  )

str(df_user_activity)
```

Correlação entre minutos e calorias:
```{r}
# Filtering data to remove NAs
df_corr <- df_user_activity %>%
  select(avg_steps, avg_calories) %>%
  filter(!is.na(avg_steps), !is.na(avg_calories))

# Pearson correlation calculation
correlation <- cor(df_corr$avg_steps, df_corr$avg_calories, method = "pearson")
print(paste("Pearson correlation between average steps and average calories:", round(correlation, 3)))

# Scatter plot with regression line
ggplot(df_corr, aes(x = avg_steps, y = avg_calories)) +
  geom_point(color = "#0073C2FF", size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "darkred", linetype = "dashed") +
  labs(
    title = "Correlation between Average Daily Steps and Calories Burned",
    x = "Average Daily Steps",
    y = "Average Daily Calories Burned"
  ) +
  theme_minimal()
```

Calorias gastas por dia da semana:
```{r}
# Optional: Order weekdays
df_daily_activity$week_day_name <- factor(
  df_daily_activity$week_day_name,
  levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")
)

# Plot: Calories burned by weekday
ggplot(df_daily_activity, aes(x = week_day_name, y = calories)) +
  geom_boxplot(fill = "#00BFC4", color = "black", alpha = 0.7) +
  stat_summary(fun = mean, geom = "point", shape = 20, size = 3, color = "red") +
  labs(
    title = "Calories Burned by Day of the Week",
    x = "Day of the Week",
    y = "Calories Burned"
  ) +
  theme_minimal()
```

### Analyze 


### Share 
### Act 



## Including Plots


